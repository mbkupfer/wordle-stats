{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c1a510-080e-4d50-971d-79aae502c1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \\\n",
    "    nltk \\\n",
    "    matplotlib \\\n",
    "    --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f2fa69-9cb5-4ea0-9499-5e08396bb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1243144-4418-424e-a4e7-5ba20203f4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download from nltk to perform tokenizing and pos tagging\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('gutenberg', quiet=True)\n",
    "nltk.download('webtext', quiet=True)\n",
    "nltk.download('brown', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06da9cc-fb0e-41ff-ba40-a82bb6601c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (pth := Path().home() / Path('english-words/words_alpha.txt')).exists():\n",
    "    with open(pth) as f:\n",
    "        english_words = f.read()\n",
    "else:\n",
    "    URL = 'https://raw.githubusercontent.com/mbkupfer/english-words/master/words_alpha.txt'\n",
    "    resp = urllib.request.urlopen(URL)\n",
    "    english_words = resp.read().decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1604055-985c-477b-b8d5-3d1c525f78fc",
   "metadata": {},
   "source": [
    "## Build out proper Wordle vocabulary\n",
    "\n",
    "Constraints:\n",
    "- only 5 letters\n",
    "- no proper nouns\n",
    "- nothing offensive (more on this one later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84764609-6746-4460-a6ff-237572f7930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_vocab = []\n",
    "for word, pos in pos_tag(word_tokenize(english_words)):\n",
    "    if len(word) == 5 and pos != 'NNP':\n",
    "        valid_vocab.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78fe1b98-61d5-48b3-89b8-8671e15a8627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 3,864,812\n",
      "Total valid wordle words: 15,842\n"
     ]
    }
   ],
   "source": [
    "print(f'Total words: {len(english_words):,}')\n",
    "valid_vocab_word_count = len(valid_vocab)\n",
    "print(f'Total valid wordle words: {valid_vocab_word_count:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "633b6a01-ea61-4642-9f39-1637db8c1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_freq_dist(vocab: List[str], n: int) -> nltk.FreqDist:\n",
    "    fdist = nltk.FreqDist()\n",
    "    for word in vocab:\n",
    "        for ngram in nltk.ngrams(word, n=n):\n",
    "            fdist[''.join(ngram)] += 1\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "372198ca-6938-4c9f-90a0-4a5249326fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 ngrams\n",
      "============\n",
      "\n",
      "1-gram\n",
      "-----------\n",
      "Total 1-grams: 79,210\n",
      "Top 5:\n",
      "a\t8,343\t10.5%\n",
      "e\t7,775\t9.8%\n",
      "s\t6,524\t8.2%\n",
      "o\t5,186\t6.5%\n",
      "r\t5,130\t6.5%\n",
      "2-gram\n",
      "-----------\n",
      "Total 2-grams: 63,368\n",
      "Top 5:\n",
      "er\t911\t1.4%\n",
      "ar\t889\t1.4%\n",
      "an\t831\t1.3%\n",
      "es\t768\t1.2%\n",
      "re\t760\t1.2%\n",
      "3-gram\n",
      "-----------\n",
      "Total 3-grams: 47,526\n",
      "Top 5:\n",
      "ing\t89\t0.2%\n",
      "ave\t88\t0.2%\n",
      "ine\t88\t0.2%\n",
      "res\t87\t0.2%\n",
      "are\t86\t0.2%\n",
      "4-gram\n",
      "-----------\n",
      "Total 4-grams: 31,684\n",
      "Top 5:\n",
      "aver\t17\t0.1%\n",
      "ills\t17\t0.1%\n",
      "ears\t14\t0.0%\n",
      "acks\t13\t0.0%\n",
      "ails\t13\t0.0%\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "Top 5 ngrams\n",
    "============\n",
    "''')\n",
    "for n in range(1, 5):\n",
    "    print(f'{n}-gram\\n-----------')\n",
    "    fdist = ngrams_freq_dist(valid_vocab, n=n)\n",
    "    cnt = fdist.total()\n",
    "    print(f'Total {n}-grams: {cnt:,}')\n",
    "    print('Top 5:')\n",
    "    top_5 = fdist.most_common(5)\n",
    "    for gram, gram_cnt in top_5:\n",
    "        print(f'{gram}\\t{gram_cnt:,}\\t{gram_cnt / cnt:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cfd93aa-8d7c-42cb-8cf0-f3f051209a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words that do not have any of the top 5 letters: 785\n",
      "First 25:\n",
      "\n",
      "['bhili', 'bibby', 'bichy', 'biddy', 'biffy', 'bifid', 'biggy', 'bight', 'bigly', 'bilbi', 'bilby', 'bilch', 'bilgy', 'bilic', 'bilin', 'billy', 'bindi', 'bingy', 'bynin', 'binit', 'binny', 'bitch', 'bitty', 'bivvy', 'bixin']\n"
     ]
    }
   ],
   "source": [
    "words_with_no_top_5_letter = [w for w in valid_vocab if not set(w) & set(['a', 'e', 's', 'o', 'r'])]\n",
    "print(f'Total words that do not have any of the top 5 letters: {len(words_with_no_top_5_letter)}')\n",
    "print(f'First 25:\\n')\n",
    "print(words_with_no_top_5_letter[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef70751-e14b-4331-ba20-cffa95a58d21",
   "metadata": {},
   "source": [
    "Not many common words in there, and in fact some of the words could be considered offensive given the context.\n",
    "\n",
    "Instead, let's use a common corpus to quickly rule out valid, but likely rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48cbca9a-0647-43ee-83e0-8c3397f716aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1bb1f62-a056-4076-a4fe-3356333a7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_words_from_corpus(vocab, corpus_words):\n",
    "    return set(vocab) & set(corpus_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0afa716a-dc30-42c3-a124-2e8cc529d23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_words_with_no_top_5 = only_words_from_corpus(words_with_no_top_5_letter, brown.words(categories='news'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79085878-6b74-4ad3-85c8-6e378fd119be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown_words_with_no_top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d059217d-3ee0-468d-8b39-3dee84869afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blind',\n",
       " 'blunt',\n",
       " 'buddy',\n",
       " 'build',\n",
       " 'built',\n",
       " 'bulky',\n",
       " 'child',\n",
       " 'civil',\n",
       " 'cubic',\n",
       " 'dying',\n",
       " 'fifth',\n",
       " 'fifty',\n",
       " 'fight',\n",
       " 'filly',\n",
       " 'filmy',\n",
       " 'fluid',\n",
       " 'flung',\n",
       " 'fully',\n",
       " 'guilt',\n",
       " 'gully',\n",
       " 'hitch',\n",
       " 'hubby',\n",
       " 'imply',\n",
       " 'jumpy',\n",
       " 'light',\n",
       " 'limit',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'lying',\n",
       " 'might',\n",
       " 'night',\n",
       " 'ninth',\n",
       " 'pitch',\n",
       " 'quick',\n",
       " 'thigh',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'tight',\n",
       " 'tying',\n",
       " 'unify',\n",
       " 'unity',\n",
       " 'until',\n",
       " 'vivid',\n",
       " 'which',\n",
       " 'windy'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_words_with_no_top_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accf823d-2c95-4af4-a8a7-87e89ad74ddf",
   "metadata": {},
   "source": [
    "Let's create a more generalized function to get hards words by eliminating all words that have a common ngram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59a0b1b3-f421-4693-8b2d-a026c749a727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'buddy',\n",
       " 'bulky',\n",
       " 'combo',\n",
       " 'comic',\n",
       " 'coyly',\n",
       " 'cubic',\n",
       " 'filmy',\n",
       " 'fluid',\n",
       " 'hubby',\n",
       " 'imply',\n",
       " 'jumpy',\n",
       " 'onion',\n",
       " 'quick',\n",
       " 'which',\n",
       " 'widow'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_hard_words(vocab, ngram_cutoffs):\n",
    "    \"\"\"Only get words that don't have a top ngram\n",
    "\n",
    "    vocab: List[str]\n",
    "        Valid words to pick from\n",
    "    ngram_cutoffs: List[int]\n",
    "        A list of numbers that correspond to the most common cutoffs for each ngram. The first element\n",
    "        corresponds to 1-grams, second is the 2-grams cutoff, etc.\n",
    "    \"\"\"\n",
    "    vocab = set(vocab) & set(brown.words(categories='news'))\n",
    "    for n, cutoff in enumerate(ngram_cutoffs, start=1):\n",
    "        fdist = ngrams_freq_dist(vocab, n)\n",
    "        top = [w for w, _ in fdist.most_common(cutoff)]\n",
    "        exclude = [w for w in vocab if set(''.join(i) for i in nltk.ngrams(w, n=n)) & set(top)]\n",
    "        vocab = vocab - set(exclude) \n",
    "    return vocab\n",
    "\n",
    "get_hard_words(valid_vocab, [5,5,5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
